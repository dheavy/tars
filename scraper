#!/usr/bin/env python
# -*- coding: utf-8 -*-


import os
import sys
import argparse
import psycopg2

from tendo import singleton
from psycopg2.extensions import AsIs
from multiprocessing.pool import ThreadPool

from mypleasure import settings
from mypleasure.tars import Tars
from mypleasure.utils import Logger


# Set up logger.
log = Logger()


def parse_args():
    """
    Parse arguments passed in console.

    Returns:
      The tuple of arguments (url, collection_id, requester)
      passed in the console.
    """
    workers_num = os.environ.get('TARS_WORKERS_NUMBER') or 1
    workers_help = """number of workers to use, %(w)s'
                   by default (ideal number
                   is twice the number of CPUs
                   available in machine')""" % {'w': workers_num}

    description = ('Fetch media content for websites via scraping, '
                   'APIs or (n)oEmbed.\n'
                   'Uses and updates the media queue.')

    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('--url', type=str, help='url of media content')
    parser.add_argument(
        '--workers',
        type=int,
        default=workers_num,
        help=workers_help
    )
    return parser.parse_args()


def run():
    """Run the scraper as a singleton instance."""

    # Ensure one and only one instance should work
    # with the proposed amount of workers.
    singleton.SingleInstance()

    # Get CLI arguments.
    options = parse_args()

    # Multiprocessing: create a pool of workers for the task.
    pool = ThreadPool(options.workers)

    # Create DB connection and cursor.
    try:
        connection_string = 'dbname={0} user={1} password={2} host={3}'.format(
            settings.DB_NAME, settings.DB_USER,
            settings.DB_PASSWORD, settings.DB_HOST
        )
        conn = psycopg2.connect(connection_string)
    except:
        log.error("Could not connect to DB")
        sys.exit()

    db = conn.cursor()

    # Create the runner instance.
    tars = Tars(db=db)

    # Run TARS in CLI mode if a `url` argument was passed in console.
    # Otherwise, prepare batches of tasks from the media queue and run them
    # using the thread pool.
    if options.url:
        log.trace('Calling TARS in console mode.')
        metadata = tars.run(None, url=options.url)
        if metadata:
            print metadata
        else:
            print 'No metadata... TARS probably failed.'
    else:
        # Prepare a set (no duplicates!) of queue elements with
        # "pending" status. Turn them into a list of processable
        # dictionaries used as argument for TARS' work.
        db.execute(
            "SELECT url, hash, requester FROM %(table)s \
             WHERE status = 'pending'",
            {'table': AsIs(settings.DB_TABLE_QUEUE)}
        )
        pending = [item for item in list(set(db.fetchall()))]
        processables = [
            {'url': elm[0], 'hash': elm[1], 'requester': elm[2]}
            for elm in pending
        ]
        pool.map(tars.run, processables)


if __name__ == '__main__':
    run()
