#!/usr/bin/env python
# -*- coding: utf-8 -*-


import os, argparse, sys, psycopg2
import mypleasure.settings as settings
from tendo import singleton
from mypleasure.tars import Tars
from mypleasure.utils import Logger
from psycopg2.extensions import AsIs
from multiprocessing.pool import ThreadPool


def parse_args():
  """Parse arguments passed in console.

  Returns:
    The tuple of arguments (url, collection_id, requester) passed in the console.
  """
  default_workers_num = os.environ.get('TARS_WORKERS_NUMBER') or 1
  default_workers_help = ('number of workers to use, ',
                          str(default_workers_num) + 'by default ',
                          '(ideal number is twice the number of CPUs ',
                          'available in machine')

  description = ('Fetch media content for websites via scraping, APIs or (n)oEmbed.\n',
                 'Uses and updates the media queue.')

  parser = argparse.ArgumentParser(description=description)
  parser.add_argument('--url', type=str, help='url of media content')
  parser.add_argument('--workers', type=int, default=default_workers_num, help=default_workers_help)
  return parser.parse_args()


def run():
  """Run the scraper as a singleton instance."""

  # Ensure one and only one instance should work
  # with the proposed amount of workers.
  lock = singleton.SingleInstance()

  # Set up logger.
  log = Logger()

  # Get CLI arguments.
  options = parse_args()

  # Multiprocessing: create a pool of workers for the task.
  pool = ThreadPool(options.workers)

  # Create DB connection and cursor.
  try:
    connection_string = 'dbname={0} user={1} password={2} host={3}'.format(settings.DB_NAME, settings.DB_USER, settings.DB_PASSWORD, settings.DB_HOST)
    conn = psycopg2.connect(connection_string)
  except:
    # log: could not connect to DB
    print("could not connect to DB")
    sys.exit()

  db = conn.cursor()

  # Prepare a list of queue elements with "pending" status.
  db.execute("SELECT * FROM %(table)s WHERE status = 'pending'", { "table": AsIs(settings.DB_TABLE_QUEUE) })
  pending = db.fetchall()
  print(pending)

  # Create the scraper instance.
  tars = Tars(db=db)


if __name__ == '__main__': run()